# Colab-ready universal EDA + mandatory standardize / dedupe / normalize pipeline
# Copy-paste entire cell into Google Colab and run.
# Installs deps, loads dataset (tries /mnt/data/retail_data_Source.csv), runs EDA + preprocessing,
# saves results and preprocess pipeline to /content/eda_output_<ts>/ and zips it.

!pip install -q pandas numpy matplotlib seaborn scipy scikit-learn statsmodels joblib

import os, sys, json, shutil, re
from datetime import datetime
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid", rc={"figure.figsize": (8,5)})
import warnings
warnings.filterwarnings("ignore")

# sklearn
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib

# statsmodels for VIF (optional)
try:
    from statsmodels.stats.outliers_influence import variance_inflation_factor
except Exception:
    variance_inflation_factor = None

# --------------------------
# Helpers: load data (auto)
# --------------------------
def load_data_auto(preferred_path="/mnt/data/retail_data_Source.csv", sheet_name=None):
    from google.colab import files
    path = None
    if preferred_path and os.path.exists(preferred_path):
        path = preferred_path
    elif os.path.exists("retail_data_Source.csv"):
        path = "retail_data_Source.csv"
    elif os.path.exists("/content/retail_data_Source.csv"):
        path = "/content/retail_data_Source.csv"
    else:
        print("File not found in default locations. Please upload your dataset now (CSV / Excel / JSON / Parquet).")
        uploaded = files.upload()
        if len(uploaded) == 0:
            raise FileNotFoundError("No file uploaded.")
        path = list(uploaded.keys())[0]
    print("Loading file:", path)
    ext = os.path.splitext(path)[1].lower()
    if ext == ".csv":
        return pd.read_csv(path, low_memory=False)
    if ext in (".xls", ".xlsx"):
        return pd.read_excel(path, sheet_name=sheet_name)
    if ext == ".json":
        try:
            return pd.read_json(path, lines=True)
        except:
            return pd.read_json(path, lines=False)
    if ext == ".parquet":
        return pd.read_parquet(path)
    # fallback
    try:
        return pd.read_csv(path, low_memory=False)
    except Exception as e:
        raise ValueError(f"Unable to read file {path}: {e}")

# --------------------------
# Output directory
# --------------------------
def make_output_dir(base_name="eda_output"):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out = f"/content/{base_name}_{ts}"
    os.makedirs(out, exist_ok=True)
    return out

# --------------------------
# Standardize column names
# --------------------------
def standardize_column_names(df):
    def clean(name):
        if not isinstance(name, str):
            name = str(name)
        name = name.strip().lower()
        # replace spaces and special chars with underscore
        name = re.sub(r'[^0-9a-z]+', '_', name)
        name = re.sub(r'_+', '_', name).strip('_')
        return name
    new_cols = [clean(c) for c in df.columns]
    df.columns = new_cols
    return df

# --------------------------
# Smart deduplication
# --------------------------
def smart_deduplicate(df):
    # heuristics: look for id-like columns
    id_like = [c for c in df.columns if re.search(r'id$|^id$|invoice|order_no|order|orderid|transaction', c)]
    if id_like:
        # choose the first id-like column which has duplicates if any
        for col in id_like:
            if df[col].notna().sum() > 0:
                before = df.shape[0]
                df = df.drop_duplicates(subset=[col], keep='first')
                method = f"dedup_by_{col}"
                after = df.shape[0]
                return df, method, before-after
    # fallback: exact-row duplicates
    before = df.shape[0]
    df = df.drop_duplicates(keep='first')
    after = df.shape[0]
    if after < before:
        return df, "dedup_by_exact_rows", before-after
    else:
        return df, "no_duplicates_found", 0

# --------------------------
# Preprocessing: standardize strings, handle missing, encode categories, scale numbers
# --------------------------
def preprocess_and_save_pipeline(df, outdir, mandatory_standardize=True, mandatory_normalize=True):
    # 1. Standardize column names (already done upstream)
    # 2. Trim string columns, fill NA for categorical
    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # Convert boolean to numeric if present
    bool_cols = df.select_dtypes(include=['bool']).columns.tolist()
    # Keep record of original numeric cols
    orig_num_cols = num_cols.copy()

    # apply trimming on object columns
    for c in cat_cols:
        try:
            df[c] = df[c].astype(str).str.strip().replace({'nan':'<<MISSING>>'}).fillna("<<MISSING>>")
        except Exception:
            df[c] = df[c].fillna("<<MISSING>>").astype(str)

    # handle bool -> int
    for c in bool_cols:
        df[c] = df[c].astype(int)
        if c not in num_cols:
            num_cols.append(c)

    # Re-evaluate numeric columns in case some numeric-like object columns exist
    # try to coerce object columns to numeric where feasible (no editing required later)
    coerced = []
    for c in cat_cols:
        # if many values look numeric, coerce
        sample = df[c].dropna().astype(str).head(500).tolist()
        numeric_like = sum(1 for v in sample if re.fullmatch(r'[-+]?\d+(\.\d+)?', v)) / max(1, len(sample))
        if numeric_like > 0.8:
            df[c] = pd.to_numeric(df[c], errors='coerce')
            coerced.append(c)
    # update lists
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()

    # 3. Build transformers
    # For numeric: produce MinMax and Standard (zscore). We'll save transformed copies; we will not overwrite original numeric columns.
    numeric_transformer_mm = MinMaxScaler()
    numeric_transformer_std = StandardScaler()

    # For categorical: LabelEncoder per column (save mapping)
    label_encoders = {}
    for c in cat_cols:
        le = LabelEncoder()
        # LabelEncoder cannot fit on NaN-like; fill with placeholder
        df[c] = df[c].fillna("<<MISSING>>").astype(str)
        try:
            le.fit(df[c].astype(str))
            label_encoders[c] = le
        except Exception:
            # fallback: map unique to integers
            uniques = list(pd.Series(df[c].astype(str).unique()))
            mapping = {v:i for i,v in enumerate(uniques)}
            # create a simple encoder-like object
            class SimpleLE:
                def __init__(self,mapping): self.mapping=mapping
                def transform(self, arr): return [self.mapping.get(str(x), -1) for x in arr]
                def fit(self, arr): pass
            label_encoders[c] = SimpleLE(mapping)
    # Fit numeric scalers on numeric columns
    X_num = df[num_cols].fillna(0).values if num_cols else np.empty((len(df),0))
    if mandatory_normalize and len(num_cols)>0:
        numeric_transformer_mm.fit(X_num)
        numeric_transformer_std.fit(X_num)

    # 4. Apply transforms and save new columns
    # Create copies in df_out to avoid destructive changes
    df_out = df.copy()

    # Add normalized columns: suffixes _minmax and _zsc
    for i, col in enumerate(num_cols):
        try:
            col_vals = df[[col]].fillna(0).values
            if mandatory_normalize:
                mm = numeric_transformer_mm.transform(col_vals).reshape(-1,)
                zsc = numeric_transformer_std.transform(col_vals).reshape(-1,)
                df_out[col + "_minmax"] = mm
                df_out[col + "_zsc"] = zsc
        except Exception as e:
            print(f"Warning: scaling failed for {col}: {e}")

    # Add encoded categorical columns with suffix _le
    for c, le in label_encoders.items():
        try:
            df_out[c + "_le"] = le.transform(df_out[c].astype(str))
        except Exception:
            # if simple object
            df_out[c + "_le"] = pd.factorize(df_out[c].astype(str))[0]

    # Save preprocessing pipeline (we'll store scaler objects + label mappings in a dict)
    preprocess_artifact = {
        "numeric_columns": num_cols,
        "categorical_columns": cat_cols,
        "minmax_scaler": numeric_transformer_mm if len(num_cols)>0 else None,
        "zscore_scaler": numeric_transformer_std if len(num_cols)>0 else None,
        "label_encoders": label_encoders
    }
    # joblib can't always serialize sklearn LabelEncoder easily if custom; save pipeline with joblib (should work)
    joblib.dump(preprocess_artifact, os.path.join(outdir, "preprocess_pipeline.pkl"))

    # 5. Save transformed dataframe
    df_out.to_csv(os.path.join(outdir, "preprocessed_full.csv"), index=False)
    # also save original numeric-only with scaled columns for analyst convenience
    summary_num = df_out[[c for c in df_out.columns if c.endswith("_minmax") or c.endswith("_zsc")]].copy()
    if not summary_num.empty:
        summary_num.to_csv(os.path.join(outdir, "numeric_transformed_preview.csv"), index=False)

    # Save label encoder mappings for reference
    le_mappings = {}
    for c, le in label_encoders.items():
        try:
            if hasattr(le, "classes_"):
                le_mappings[c] = list(map(str, le.classes_))
            elif hasattr(le, "mapping"):
                le_mappings[c] = le.mapping
            else:
                le_mappings[c] = "<unknown_encoder_type>"
        except Exception:
            le_mappings[c] = "<save_failed>"
    with open(os.path.join(outdir, "label_encoder_mappings.json"), "w") as f:
        json.dump(le_mappings, f, indent=2)

    return df_out, preprocess_artifact

# --------------------------
# VIF (optional)
# --------------------------
def calculate_vif(df, outdir, max_features=50):
    num = df.select_dtypes(include=[np.number]).copy()
    if num.shape[1] < 2 or variance_inflation_factor is None:
        return None
    cols = num.columns.tolist()[:max_features]
    X = num[cols].fillna(0).values
    vif_list = []
    for i in range(X.shape[1]):
        try:
            vif_val = variance_inflation_factor(X, i)
        except Exception:
            vif_val = None
        vif_list.append((cols[i], float(vif_val) if vif_val is not None else None))
    vif_df = pd.DataFrame(vif_list, columns=["feature","VIF"]).sort_values("VIF", ascending=False)
    vif_df.to_csv(os.path.join(outdir, "vif.csv"), index=False)
    return vif_df

# --------------------------
# EDA elements: missing, univariate, bivariate
# --------------------------
def save_basic_info(df, outdir):
    info = {
        "shape": df.shape,
        "columns": df.columns.tolist(),
        "dtypes": df.dtypes.apply(lambda x:str(x)).to_dict(),
        "memory_bytes": int(df.memory_usage(deep=True).sum())
    }
    with open(os.path.join(outdir, "basic_info.json"), "w") as f:
        json.dump(info, f, indent=2)
    df.describe(include=[np.number]).T.to_csv(os.path.join(outdir, "describe_numeric.csv"))
    df.describe(include=['object','category','bool']).T.to_csv(os.path.join(outdir, "describe_categorical.csv"))
    return info

def missing_and_duplicates_report(df, outdir):
    miss = df.isnull().sum().sort_values(ascending=False)
    miss_pct = (df.isnull().mean()*100).sort_values(ascending=False)
    missing_df = pd.concat([miss, miss_pct], axis=1)
    missing_df.columns = ["missing_count", "missing_percent"]
    missing_df.to_csv(os.path.join(outdir, "missing_report.csv"))
    # heatmap
    try:
        sample = df if df.shape[0] <= 2000 else df.sample(2000, random_state=1)
        plt.figure(figsize=(12,5))
        sns.heatmap(sample.isnull(), cbar=False)
        plt.title("Missing values map (sampled)")
        plt.tight_layout()
        plt.savefig(os.path.join(outdir, "missing_heatmap.png"))
        plt.close()
    except Exception as e:
        print("Warning creating missing heatmap:", e)
    dup_count = int(df.duplicated().sum())
    with open(os.path.join(outdir, "duplicates_count.txt"), "w") as f:
        f.write(str(dup_count))
    return missing_df, dup_count

def univariate_plots(df, outdir, max_unique_cat=50):
    os.makedirs(os.path.join(outdir, "univariate"), exist_ok=True)
    for col in df.columns:
        try:
            ser = df[col]
            fname = os.path.join(outdir, "univariate", f"{col}_univariate.png")
            plt.figure(figsize=(8,4))
            if pd.api.types.is_numeric_dtype(ser):
                ser.dropna().hist(bins=30)
                plt.title(f"{col} (numeric) — histogram")
            else:
                vc = ser.fillna("<<MISSING>>").value_counts()
                if vc.shape[0] <= max_unique_cat:
                    vc.plot(kind="bar")
                    plt.title(f"{col} (categorical) — counts")
                else:
                    vc.head(30).plot(kind="bar")
                    plt.title(f"{col} (categorical) — top 30 counts")
                plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()
        except Exception as e:
            # ignore failures for weird columns
            pass

def bivariate_plots(df, outdir):
    os.makedirs(os.path.join(outdir, "bivariate"), exist_ok=True)
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(num_cols) < 2:
        return
    corr = df[num_cols].corr().abs().unstack().reset_index()
    corr.columns = ["x","y","corr"]
    corr = corr[corr['x']!=corr['y']].sort_values("corr", ascending=False).drop_duplicates(subset=['corr'])
    for _, row in corr.head(6).iterrows():
        x,y = row['x'], row['y']
        try:
            sample = df[[x,y]].dropna()
            if sample.shape[0] > 5000:
                sample = sample.sample(5000, random_state=1)
            plt.figure(figsize=(6,4))
            sns.scatterplot(x=sample[x], y=sample[y], alpha=0.5)
            plt.title(f"{x} vs {y} (corr={row['corr']:.2f})")
            plt.tight_layout()
            plt.savefig(os.path.join(outdir, "bivariate", f"{x}_vs_{y}.png"))
            plt.close()
        except Exception:
            pass

# --------------------------
# Runner
# --------------------------
outdir = make_output_dir()
print("Output directory:", outdir)

# Load dataset
try:
    df = load_data_auto(preferred_path="/mnt/data/retail_data_Source.csv")
except Exception as e:
    print("Auto-load failed:", e)
    df = load_data_auto(preferred_path=None)

print("Initial rows,cols:", df.shape)

# Standardize column names
df = standardize_column_names(df)

# Save head for reference
df.head(10).to_csv(os.path.join(outdir, "head_preview.csv"), index=False)

# Save basic info pre-processing
save_basic_info(df, outdir)

# Missing and duplicates (before dedupe)
missing_df_before, dup_before = missing_and_duplicates_report(df, outdir)
with open(os.path.join(outdir, "pre_dedupe_info.json"), "w") as f:
    json.dump({"missing_top": missing_df_before.head(20).to_dict(), "duplicates": int(dup_before)}, f, indent=2)

# Smart deduplicate
df_deduped, dedup_method, n_removed = smart_deduplicate(df)
with open(os.path.join(outdir, "dedup_method.txt"), "w") as f:
    f.write(f"{dedup_method}\nremoved_rows:{int(n_removed)}\n")
print(f"Deduplication method: {dedup_method}, rows removed: {int(n_removed)}")

# Save post-dedupe snapshot
df_deduped.shape
df_deduped.head(5).to_csv(os.path.join(outdir, "head_post_dedupe.csv"), index=False)

# Run mandatory preprocessing (standardize strings, categorical encode, normalize numbers)
df_preprocessed, preprocess_artifact = preprocess_and_save_pipeline(df_deduped, outdir)

# Save missing and dup info after preprocessing
missing_df_after, dup_after = missing_and_duplicates_report(df_preprocessed, outdir)
with open(os.path.join(outdir, "post_processing_info.json"), "w") as f:
    json.dump({
        "missing_top": missing_df_after.head(20).to_dict(),
        "duplicates_after": int(dup_after),
        "preprocess_numeric_cols": preprocess_artifact.get("numeric_columns"),
        "preprocess_categorical_cols": preprocess_artifact.get("categorical_columns")
    }, f, indent=2)

# EDA plots
univariate_plots(df_preprocessed, outdir)
bivariate_plots(df_preprocessed, outdir)

# VIF (optional, may be heavy)
vif_df = calculate_vif(df_preprocessed, outdir)
if vif_df is not None:
    print("VIF computed for top features. Saved to vif.csv")

# Save summarised files
df_preprocessed.shape
df_preprocessed.to_csv(os.path.join(outdir, "preprocessed_saved.csv"), index=False)

# Short text summary
summary_lines = [
    f"Original shape: {df.shape}",
    f"After dedupe ({dedup_method}): {df_deduped.shape}, rows_removed: {int(n_removed)}",
    f"Preprocessed shape (with new transformed cols): {df_preprocessed.shape}",
    f"Numeric columns transformed (minmax + zscore): {preprocess_artifact.get('numeric_columns')}",
    f"Categorical columns encoded: {preprocess_artifact.get('categorical_columns')}"
]
with open(os.path.join(outdir, "summary.txt"), "w") as f:
    f.write("\n".join(map(str, summary_lines)))
print("\n".join(summary_lines))

# Zip outputs for download
zip_name = outdir + ".zip"
shutil.make_archive(outdir, 'zip', outdir)
print("\nZipped output to:", zip_name)

# Provide Colab download link
from IPython.display import HTML, display
display(HTML(f'<a href="{zip_name}" target="_blank">Download EDA + Preprocessing outputs (zip)</a>'))

# List top-level files
print("\nTop-level files created:")
print(sorted(os.listdir(outdir)))
