# Colab-ready pipeline: dedupe, null-handling, regex validation, outlier removal, dtype downcast,
# unsupervised feature selection (low variance + high-corr), encode + normalize, save artifacts (NO PLOTS, NO VIF)
!pip install -q pandas numpy scipy scikit-learn joblib

import os, sys, json, shutil, re
from datetime import datetime
import pandas as pd, numpy as np
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.feature_selection import VarianceThreshold
import joblib

# --------------------------
# CONFIG (tweak these)
# --------------------------
COL_MISSING_THRESH_COL_DROP = 0.5     # drop columns with >50% missing
ROW_MISSING_THRESH_DROP = 0.5         # drop rows with >50% missing
SAMPLE_FOR_TYPE_INFERENCE = 500
EMAIL_REGEX = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")
OUTLIER_IQR_K = 1.5                   # IQR multiplier for outlier detection (1.5 standard)
LOW_VARIANCE_THRESHOLD = 0.0          # variance threshold (0 removes constant cols)
CORR_THRESHOLD = 0.95                 # if absolute corr > this, drop one of the pair
DOWNCAST_INTS = True
DOWNCAST_FLOATS = True

# --------------------------
# Utilities: load / outdir
# --------------------------
def load_data_auto(preferred_path="/mnt/data/retail_data_Source.csv", sheet_name=None):
    from google.colab import files
    path = None
    if preferred_path and os.path.exists(preferred_path):
        path = preferred_path
    elif os.path.exists("retail_data_Source.csv"):
        path = "retail_data_Source.csv"
    elif os.path.exists("/content/retail_data_Source.csv"):
        path = "/content/retail_data_Source.csv"
    else:
        print("File not found in default locations. Please upload your dataset now (CSV / Excel / JSON / Parquet).")
        uploaded = files.upload()
        if len(uploaded) == 0:
            raise FileNotFoundError("No file uploaded.")
        path = list(uploaded.keys())[0]
    print("Loading file:", path)
    ext = os.path.splitext(path)[1].lower()
    if ext == ".csv":
        return pd.read_csv(path, low_memory=False)
    if ext in (".xls", ".xlsx"):
        return pd.read_excel(path, sheet_name=sheet_name)
    if ext == ".json":
        try:
            return pd.read_json(path, lines=True)
        except:
            return pd.read_json(path)
    if ext == ".parquet":
        return pd.read_parquet(path)
    return pd.read_csv(path, low_memory=False)

def make_output_dir(base_name="eda_output"):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out = f"/content/{base_name}_{ts}"
    os.makedirs(out, exist_ok=True)
    return out

# --------------------------
# Name standardization
# --------------------------
def standardize_column_names(df):
    df.columns = [re.sub(r'[^0-9a-z]+','_', str(c).strip().lower()).strip('_') for c in df.columns]
    return df

# --------------------------
# Detect date/time/email columns
# --------------------------
def detect_special_columns(df):
    date_cols=[]; time_cols=[]; email_cols=[]
    for c in df.columns:
        lc = c.lower()
        if re.search(r'date|dob|day|timestamp|created_at|order_date', lc): date_cols.append(c)
        if re.search(r'time|sec|hour|min|duration', lc): time_cols.append(c)
        if re.search(r'email|e_mail|mail', lc): email_cols.append(c)
    # content-based
    for c in df.columns:
        if c in date_cols+time_cols+email_cols: continue
        sample = df[c].dropna().astype(str).head(SAMPLE_FOR_TYPE_INFERENCE).tolist()
        if not sample: continue
        if sum(1 for v in sample if re.search(r'\d{1,4}[-/ ]\d{1,2}[-/ ]\d{1,4}', v))/len(sample) > 0.6:
            date_cols.append(c); continue
        if sum(1 for v in sample if re.search(r'\d{1,2}:\d{2}', v))/len(sample) > 0.6:
            time_cols.append(c); continue
        if sum(1 for v in sample if EMAIL_REGEX.match(v.strip()))/len(sample) > 0.6:
            email_cols.append(c); continue
    return sorted(set(date_cols)), sorted(set(time_cols)), sorted(set(email_cols))

# --------------------------
# Dedupe (drop duplicates immediately)
# --------------------------
def smart_deduplicate_drop(df, outdir):
    before = df.shape[0]
    df = df.drop_duplicates(keep='first').reset_index(drop=True)
    removed = before - df.shape[0]
    id_like = [c for c in df.columns if re.search(r'\b(id|order|invoice|transaction|order_no|orderid)\b', c)]
    for c in id_like:
        if c in df.columns:
            b = df.shape[0]
            df = df.drop_duplicates(subset=[c], keep='first').reset_index(drop=True)
            removed += (b - df.shape[0])
    with open(os.path.join(outdir, "dedup_method.txt"), "w") as f:
        f.write(f"rows_removed:{removed}\nid_like_cols:{id_like}")
    return df, removed

# --------------------------
# Null handling (drop cols, impute, drop rows)
# --------------------------
def handle_nulls(df, outdir):
    miss_frac = df.isnull().mean()
    cols_to_drop = miss_frac[miss_frac > COL_MISSING_THRESH_COL_DROP].index.tolist()
    df = df.drop(columns=cols_to_drop)
    # numeric -> median
    for c in df.select_dtypes(include=[np.number]).columns:
        if df[c].isnull().any():
            df[c] = df[c].fillna(df[c].median())
    # object -> mode or placeholder
    for c in df.select_dtypes(include=['object','category']).columns:
        if df[c].isnull().any():
            mode = df[c].mode(dropna=True)
            fill = mode.iloc[0] if not mode.empty else "<<MISSING>>"
            df[c] = df[c].fillna(fill)
    # drop rows with too many missing
    row_miss_frac = df.isnull().mean(axis=1)
    before = df.shape[0]
    df = df.loc[row_miss_frac <= ROW_MISSING_THRESH_DROP].reset_index(drop=True)
    rows_dropped = before - df.shape[0]
    with open(os.path.join(outdir,"null_handling.txt"), "w") as f:
        f.write(json.dumps({"cols_dropped":cols_to_drop, "rows_dropped":rows_dropped}, indent=2))
    return df, cols_to_drop, rows_dropped

# --------------------------
# Validate/fix email, date, time columns
# --------------------------
def validate_and_fix_special_cols(df, date_cols, time_cols, email_cols, outdir):
    issues = {"email":{}, "date":{}, "time":{}}
    for c in email_cols:
        if c not in df.columns: continue
        valid = df[c].astype(str).apply(lambda x: bool(EMAIL_REGEX.match(str(x).strip())) if str(x).strip().lower()!='nan' else False)
        df[c + "_email_valid"] = valid
        issues["email"][c] = int((~valid).sum())
        df.loc[~valid, c] = np.nan
    for c in date_cols:
        if c not in df.columns: continue
        parsed = pd.to_datetime(df[c], errors='coerce', infer_datetime_format=True)
        # try dayfirst if many NaT
        nat_frac = parsed.isna().mean()
        if nat_frac > 0.4:
            parsed_try = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)
            if parsed_try.isna().mean() < nat_frac:
                parsed = parsed_try
        df[c + "_dt"] = parsed
        df[c + "_date_valid"] = parsed.notna()
        issues["date"][c] = int(parsed.isna().sum())
        df.loc[parsed.isna(), c] = np.nan
    for c in time_cols:
        if c not in df.columns: continue
        parsed = pd.to_datetime(df[c], errors='coerce', infer_datetime_format=True)
        parsed_time = parsed.dt.time
        df[c + "_time_parsed"] = parsed_time
        df[c + "_time_valid"] = parsed_time.notna()
        issues["time"][c] = int(parsed_time.isna().sum())
        df.loc[pd.isna(parsed_time), c] = np.nan
    with open(os.path.join(outdir,"special_column_issues.json"), "w") as f:
        json.dump(issues, f, indent=2)
    return df, issues

# --------------------------
# Outlier detection & removal (IQR method) â€” removes rows with any numeric column outlier
# --------------------------
def remove_outliers_iqr(df, outdir, k=OUTLIER_IQR_K):
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        with open(os.path.join(outdir,"outlier_removal.txt"), "w") as f:
            f.write("No numeric columns to check for outliers.")
        return df, 0
    lows = df[num_cols].quantile(0.25)
    highs = df[num_cols].quantile(0.75)
    iqr = highs - lows
    lower_bound = lows - k * iqr
    upper_bound = highs + k * iqr
    # boolean mask: True for rows that are NOT outliers for all columns
    mask = pd.Series(True, index=df.index)
    for c in num_cols:
        # skip zero-IQR columns
        if iqr[c] == 0 or pd.isna(iqr[c]):
            continue
        mask = mask & (df[c] >= lower_bound[c]) & (df[c] <= upper_bound[c])
    before = df.shape[0]
    df_clean = df.loc[mask].reset_index(drop=True)
    removed = before - df_clean.shape[0]
    with open(os.path.join(outdir, "outlier_removal.txt"), "w") as f:
        f.write(json.dumps({"method":"IQR", "k":k, "rows_before":before, "rows_after":df_clean.shape[0], "rows_removed":removed}, indent=2))
    return df_clean, int(removed)

# --------------------------
# Data type downcast to save memory
# --------------------------
def downcast_dtypes(df, outdir, down_ints=DOWNCAST_INTS, down_floats=DOWNCAST_FLOATS):
    original_memory = df.memory_usage(deep=True).sum()
    for c in df.columns:
        col = df[c]
        if down_ints and pd.api.types.is_integer_dtype(col):
            df[c] = pd.to_numeric(col, downcast='integer')
        if down_floats and pd.api.types.is_float_dtype(col):
            df[c] = pd.to_numeric(col, downcast='float')
    new_memory = df.memory_usage(deep=True).sum()
    with open(os.path.join(outdir, "dtype_downcast_log.txt"), "w") as f:
        f.write(json.dumps({"memory_before":int(original_memory), "memory_after":int(new_memory), "bytes_saved":int(original_memory-new_memory)}, indent=2))
    return df

# --------------------------
# Feature selection (unsupervised): low variance + high correlation pruning
# --------------------------
def feature_selection_unsupervised(df, outdir, var_thresh=LOW_VARIANCE_THRESHOLD, corr_thresh=CORR_THRESHOLD):
    # Work only on numeric features for variance & correlation selection
    num = df.select_dtypes(include=[np.number]).copy()
    removed = {"low_variance": [], "high_correlation": []}

    # 1) Low variance removal (VarianceThreshold)
    if num.shape[1] > 0:
        vt = VarianceThreshold(threshold=var_thresh)
        try:
            vt.fit(num.fillna(0))
            support = vt.get_support(indices=True)
            kept_cols = [num.columns[i] for i in support]
            low_var_cols = [c for c in num.columns if c not in kept_cols]
            removed["low_variance"].extend(low_var_cols)
            df = df.drop(columns=low_var_cols, errors='ignore')
        except Exception:
            # fallback no-op
            pass

    # 2) High correlation pruning (absolute corr)
    num = df.select_dtypes(include=[np.number]).copy()
    if num.shape[1] > 1:
        corr = num.corr().abs()
        # upper triangle
        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
        to_drop = set()
        for col in upper.columns:
            high_corr_cols = upper.index[upper[col] > corr_thresh].tolist()
            for rc in high_corr_cols:
                # choose to drop the column with higher mean absolute correlation
                mean_corr_col = corr[col].drop(col).mean()
                mean_corr_rc = corr[rc].drop(rc).mean()
                drop_col = col if mean_corr_col >= mean_corr_rc else rc
                to_drop.add(drop_col)
        removed["high_correlation"].extend(sorted(list(to_drop)))
        df = df.drop(columns=list(to_drop), errors='ignore')

    with open(os.path.join(outdir, "feature_selection_removed.json"), "w") as f:
        json.dump(removed, f, indent=2)

    return df, removed

# --------------------------
# Preprocess: standardize strings, encode categories, normalize numbers
# --------------------------
def preprocess_and_save_pipeline(df, outdir, mandatory_normalize=True):
    # Trim strings
    for c in df.select_dtypes(include=['object','category']).columns:
        df[c] = df[c].astype(str).str.strip().replace({'nan':'<<MISSING>>'}).fillna("<<MISSING>>")

    # convert numeric-like strings to numeric where it makes sense
    for c in df.select_dtypes(include=['object']).columns:
        sample = df[c].dropna().astype(str).head(200).tolist()
        if not sample: continue
        numeric_like = sum(1 for v in sample if re.fullmatch(r'[-+]?\d+(\.\d+)?', v)) / len(sample)
        if numeric_like > 0.8:
            df[c] = pd.to_numeric(df[c], errors='coerce')

    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # Label encode categories
    label_encoders = {}
    for c in cat_cols:
        le = LabelEncoder()
        df[c] = df[c].fillna("<<MISSING>>").astype(str)
        try:
            le.fit(df[c])
            df[c + "_le"] = le.transform(df[c])
            label_encoders[c] = le
        except Exception:
            df[c + "_le"] = pd.factorize(df[c])[0]

    # Scale numeric columns (minmax + zscore)
    mm = MinMaxScaler()
    ss = StandardScaler()
    if num_cols:
        X = df[num_cols].fillna(0).values
        mm.fit(X)
        ss.fit(X)
        for c in num_cols:
            arr = df[[c]].fillna(0).values
            try:
                df[c + "_minmax"] = mm.transform(arr).reshape(-1,)
                df[c + "_zsc"] = ss.transform(arr).reshape(-1,)
            except Exception:
                pass

    # Save pipeline artifact
    artifact = {"numeric_columns": num_cols, "categorical_columns": cat_cols, "minmax_scaler": mm, "zscore_scaler": ss, "label_encoders": label_encoders}
    joblib.dump(artifact, os.path.join(outdir, "preprocess_pipeline.pkl"))

    # Save transformed df
    df.to_csv(os.path.join(outdir, "preprocessed_full.csv"), index=False)
    # label mappings
    mappings = {}
    for k,v in label_encoders.items():
        try:
            mappings[k] = list(map(str, v.classes_))
        except Exception:
            mappings[k] = "<unable_to_dump_classes>"
    with open(os.path.join(outdir, "label_mappings.json"), "w") as f:
        json.dump(mappings, f, indent=2)

    return df, artifact

# --------------------------
# Runner
# --------------------------
outdir = make_output_dir()
print("Output directory:", outdir)

# Load data
df = load_data_auto()
print("Initial shape:", df.shape)

# Standardize column names
df = standardize_column_names(df)

# Save head
df.head(5).to_csv(os.path.join(outdir, "head_before.csv"), index=False)

# Detect special columns
date_cols, time_cols, email_cols = detect_special_columns(df)
with open(os.path.join(outdir, "detected_special_columns.json"), "w") as f:
    json.dump({"date_cols":date_cols, "time_cols":time_cols, "email_cols":email_cols}, f, indent=2)
print("Detected date/time/email cols saved.")

# Deduplicate
df, dedup_removed = smart_deduplicate_drop(df, outdir)
print("Duplicates removed:", dedup_removed)

# Validate/fix special cols
df, special_issues = validate_and_fix_special_cols(df, date_cols, time_cols, email_cols, outdir)
print("Special columns validated.")

# Null handling
df, cols_dropped_missing, rows_dropped_missing = handle_nulls(df, outdir)
print("Null handling done. cols_dropped:", cols_dropped_missing, "rows_dropped:", rows_dropped_missing)

# Outlier removal
df, outliers_removed = remove_outliers_iqr(df, outdir, k=OUTLIER_IQR_K)
print("Outliers removed (rows):", outliers_removed)

# Downcast dtypes
df = downcast_dtypes(df, outdir)
print("Dtypes downcasted where possible. New memory info in dtype_downcast_log.txt")

# Feature selection (unsupervised)
df, removed_features = feature_selection_unsupervised(df, outdir, var_thresh=LOW_VARIANCE_THRESHOLD, corr_thresh=CORR_THRESHOLD)
print("Feature selection complete. Removed:", removed_features)

# Preprocess (encode + normalize)
df_preprocessed, artifact = preprocess_and_save_pipeline(df, outdir)
print("Preprocessing (encoding + normalization) saved.")

# Final save
final_csv = os.path.join(outdir, "final_processed_data.csv")
df_preprocessed.to_csv(final_csv, index=False)

# Summary file
summary = {
    "initial_shape": None,
    "after_dedup_removed": int(dedup_removed),
    "after_nulls_cols_dropped": cols_dropped_missing,
    "after_nulls_rows_dropped": int(rows_dropped_missing),
    "outliers_removed": int(outliers_removed),
    "removed_features": removed_features,
    "final_shape": list(df_preprocessed.shape),
    "detected_special_columns": {"date": date_cols, "time": time_cols, "email": email_cols},
    "special_issues": special_issues
}
summary["initial_shape"] = list(pd.read_csv(os.path.join(outdir, "head_before.csv")).shape) if os.path.exists(os.path.join(outdir, "head_before.csv")) else None
with open(os.path.join(outdir, "summary.json"), "w") as f:
    json.dump(summary, f, indent=2)

# Zip outputs
zip_path = outdir + ".zip"
shutil.make_archive(outdir, 'zip', outdir)

from IPython.display import HTML, display
display(HTML(f'<a href="{zip_path}" target="_blank">Download processed data & artifacts (zip)</a>'))

print("Done. Final processed CSV:", final_csv)
