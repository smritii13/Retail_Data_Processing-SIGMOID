#!/usr/bin/env python3
"""
universal_eda.py

Usage:
    python universal_eda.py --file path/to/data.csv
    python universal_eda.py --file path/to/data.xlsx --sheet Sheet1
    python universal_eda.py --file path/to/data.json
    python universal_eda.py --file path/to/data.parquet

Output:
    ./eda_output_<timestamp>/  -> contains summary files and plot PNGs
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.exceptions import NotFittedError
from statsmodels.stats.outliers_influence import variance_inflation_factor
import json
import warnings
warnings.filterwarnings("ignore")
sns.set(style="whitegrid", rc={"figure.figsize": (8,5)})

def load_data(path, sheet_name=None):
    path = str(path)
    if path.lower().endswith(".csv"):
        return pd.read_csv(path)
    if path.lower().endswith((".xls", ".xlsx")):
        return pd.read_excel(path, sheet_name=sheet_name)
    if path.lower().endswith(".json"):
        return pd.read_json(path, lines=False)
    if path.lower().endswith(".parquet"):
        return pd.read_parquet(path)
    raise ValueError("Unsupported file type. Supported: csv, xls, xlsx, json, parquet")

def make_output_dir(base_name="eda_output"):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out = f"{base_name}_{ts}"
    os.makedirs(out, exist_ok=True)
    return out

def basic_info(df, outdir):
    info = {
        "shape": df.shape,
        "columns": df.columns.tolist(),
        "dtypes": df.dtypes.apply(lambda x: str(x)).to_dict(),
        "memory_bytes": df.memory_usage(deep=True).sum()
    }
    with open(os.path.join(outdir, "basic_info.json"), "w") as f:
        json.dump(info, f, indent=2)
    # descriptive stats
    desc_num = df.describe(include=[np.number]).T
    desc_cat = df.describe(include=['object', 'category', 'bool']).T
    desc_num.to_csv(os.path.join(outdir, "describe_numeric.csv"))
    desc_cat.to_csv(os.path.join(outdir, "describe_categorical.csv"))
    return info

def missing_and_duplicates(df, outdir):
    miss = df.isnull().sum().sort_values(ascending=False)
    miss_pct = (df.isnull().mean()*100).sort_values(ascending=False)
    missing_df = pd.concat([miss, miss_pct], axis=1)
    missing_df.columns = ["missing_count", "missing_percent"]
    missing_df.to_csv(os.path.join(outdir, "missing_report.csv"))
    # heatmap (sample if too many rows)
    try:
        sample = df if df.shape[0] <= 2000 else df.sample(2000, random_state=1)
        plt.figure(figsize=(12,6))
        sns.heatmap(sample.isnull(), cbar=False)
        plt.title("Missing values map (sampled if >2000 rows)")
        plt.tight_layout()
        plt.savefig(os.path.join(outdir, "missing_heatmap.png"))
        plt.close()
    except Exception as e:
        print("Warning: can't draw missing heatmap:", e)
    dup_count = int(df.duplicated().sum())
    with open(os.path.join(outdir, "duplicates.txt"), "w") as f:
        f.write(str(dup_count))
    return missing_df, dup_count

def encode_for_vif(df_numeric):
    # safe encoder for any numeric-ish cols
    return df_numeric.fillna(0)

def calculate_vif(df, outdir, thresh_cols=50):
    num = df.select_dtypes(include=[np.number])
    if num.shape[1] == 0:
        return None
    X = encode_for_vif(num)
    # if too many features, compute for first thresh_cols only (avoid heavy compute)
    cols = X.columns.tolist()
    if len(cols) > thresh_cols:
        cols = cols[:thresh_cols]
        X = X[cols]
    try:
        vif_data = []
        for i in range(X.shape[1]):
            vif = variance_inflation_factor(X.values, i)
            vif_data.append((X.columns[i], float(vif)))
        vif_df = pd.DataFrame(vif_data, columns=["feature", "VIF"]).sort_values("VIF", ascending=False)
        vif_df.to_csv(os.path.join(outdir, "vif.csv"), index=False)
        return vif_df
    except Exception as e:
        print("VIF calculation failed:", e)
        return None

def correlation_matrix(df, outdir):
    num = df.select_dtypes(include=[np.number])
    if num.shape[1] < 2:
        return None
    corr = num.corr()
    corr.to_csv(os.path.join(outdir, "correlation_matrix.csv"))
    plt.figure(figsize=(10,8))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", square=True)
    plt.title("Numeric feature correlation")
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, "correlation_heatmap.png"))
    plt.close()
    return corr

def univariate_plots(df, outdir, max_unique_cat=50):
    os.makedirs(os.path.join(outdir, "univariate"), exist_ok=True)
    for col in df.columns:
        try:
            ser = df[col]
            fname = os.path.join(outdir, "univariate", f"{col}_univariate.png")
            plt.figure(figsize=(8,4))
            if pd.api.types.is_numeric_dtype(ser):
                ser.dropna().hist(bins=30)
                plt.title(f"{col} (numeric) — histogram")
                plt.xlabel(col)
                plt.ylabel("count")
            else:
                # categorical
                vc = ser.fillna("<<MISSING>>").value_counts()
                if vc.shape[0] <= max_unique_cat:
                    vc.plot(kind="bar")
                    plt.title(f"{col} (categorical) — counts")
                    plt.xticks(rotation=45, ha='right')
                else:
                    # too many categories: plot top 30
                    vc.head(30).plot(kind="bar")
                    plt.title(f"{col} (categorical) — top 30 counts")
                    plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()
        except Exception as e:
            print(f"Warning: univariate plot failed for {col}: {e}")

def bivariate_checks(df, outdir, sample_n=5000):
    os.makedirs(os.path.join(outdir, "bivariate"), exist_ok=True)
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(num_cols) >= 2:
        # scatter for top pairs by correlation
        corr = df[num_cols].corr().abs().unstack().reset_index()
        corr.columns = ["x","y","corr"]
        corr = corr[corr['x']!=corr['y']].sort_values("corr", ascending=False).drop_duplicates(subset=['corr'])
        for i,row in corr.head(6).iterrows():
            x,y = row['x'], row['y']
            try:
                sample = df[[x,y]].dropna()
                if sample.shape[0] > sample_n:
                    sample = sample.sample(sample_n, random_state=1)
                plt.figure(figsize=(6,4))
                sns.scatterplot(x=sample[x], y=sample[y], alpha=0.6)
                plt.title(f"Scatter: {x} vs {y} (corr={row['corr']:.2f})")
                plt.tight_layout()
                plt.savefig(os.path.join(outdir, "bivariate", f"{x}_vs_{y}.png"))
                plt.close()
            except Exception as e:
                print("Warning bivariate plot failed:", e)

def detect_outliers_iqr(df, outdir):
    num = df.select_dtypes(include=[np.number])
    if num.shape[1] == 0:
        return None
    outlier_summary = []
    for col in num.columns:
        series = num[col].dropna()
        if series.empty:
            continue
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outliers = series[(series < lower) | (series > upper)]
        outlier_summary.append({
            "feature": col,
            "n_outliers": int(outliers.shape[0]),
            "pct_outliers": float(outliers.shape[0] / max(1, series.shape[0]) * 100)
        })
    out_df = pd.DataFrame(outlier_summary).sort_values("pct_outliers", ascending=False)
    out_df.to_csv(os.path.join(outdir, "outlier_summary.csv"), index=False)
    return out_df

def quick_feature_types(df, outdir):
    types = {
        "numeric": df.select_dtypes(include=[np.number]).columns.tolist(),
        "categorical": df.select_dtypes(include=['object','category','bool']).columns.tolist(),
        "datetime_like": [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])],
    }
    with open(os.path.join(outdir, "feature_types.json"), "w") as f:
        json.dump(types, f, indent=2)
    return types

def save_sample(df, outdir, n=1000):
    sample = df if df.shape[0] <= n else df.sample(n, random_state=1)
    sample.to_csv(os.path.join(outdir, "sample.csv"), index=False)

def small_text_summary(df, missing_df, dup_count, outdir):
    lines = []
    lines.append(f"Rows,Cols: {df.shape}")
    lines.append(f"Duplicates: {dup_count}")
    top_missing = missing_df[missing_df['missing_count']>0].head(10)
    lines.append("Top missing columns (count, percent):")
    for idx,row in top_missing.iterrows():
        lines.append(f"  {idx}: {row['missing_count']} ({row['missing_percent']:.2f}%)")
    with open(os.path.join(outdir, "summary.txt"), "w") as f:
        f.write("\n".join(lines))
    return "\n".join(lines)

def run_full_eda(path, sheet_name=None, outdir=None):
    df = load_data(path, sheet_name=sheet_name)
    outdir = outdir or make_output_dir()
    print("Output directory:", outdir)

    info = basic_info(df, outdir)
    missing_df, dup_count = missing_and_duplicates(df, outdir)
    types = quick_feature_types(df, outdir)
    univariate_plots(df, outdir)
    bivariate_checks(df, outdir)
    corr = correlation_matrix(df, outdir)
    vif = calculate_vif(df, outdir)
    outliers = detect_outliers_iqr(df, outdir)
    save_sample(df, outdir)
    summary_text = small_text_summary(df, missing_df, dup_count, outdir)

    print("EDA finished. Summary:")
    print(summary_text)
    return outdir

def parse_args():
    p = argparse.ArgumentParser(description="Universal EDA runner")
    p.add_argument("--file", "-f", required=True, help="Path to dataset")
    p.add_argument("--sheet", "-s", default=None, help="Excel sheet name (optional)")
    p.add_argument("--out", "-o", default=None, help="Output directory (optional)")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    try:
        outdir = run_full_eda(args.file, sheet_name=args.sheet, outdir=args.out)
        print("All outputs saved in:", outdir)
    except Exception as e:
        print("EDA failed:", e)
        sys.exit(1)
